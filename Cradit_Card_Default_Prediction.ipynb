{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "mDgbUHAGgjLW",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "g-ATYxFrGrvw",
        "578E2V7j08f6",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "OB4l2ZhMeS1U",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rushabhbhagat08/Cradit-Card-Default-Prediction/blob/main/Cradit_Card_Default_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member -** Rushabh Anilrao Bhagat"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is details of credit card holders of an \"important bank in Taiwan\" for the period April to September, 2005. The features available include some basic customer demographics (gender, education, marital status and age), available credit line, their history of payment/default for the six months mentioned (Apr--Sep '05), their bill amounts and their payment amounts for that period and a binary target variable indicating default the following month.\n",
        "\n",
        "The data was originally studied by the authors of the paper: Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480.\n",
        "\n",
        "I perform some EDA to understand the data and clean the data, engineer relevant features, build predictive models to predict default and perform some statistical analyses to obtain a greater understanding of the features and their interactions. I finish with some business case scenarios where the predictive model could be applied."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJBr0RTZcw8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from numpy import math\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from scipy import stats\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn import metrics  \n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "# from sklearn.metrics import plot_roc_curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "# from sklearn.metrics import plot_precision_recall_curve\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "\n",
        "from pprint import pprint\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade xlrd"
      ],
      "metadata": {
        "id": "ng-ArUDdaOgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YPQz2chSYb3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "file_path=('/content/drive/MyDrive/Classification_project/Credit card default prediction /default of credit card clients (1).xls')\n",
        "cradit_crd_df=pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "riP6gv8bYnGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "cradit_crd_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arranged datasets "
      ],
      "metadata": {
        "id": "3b3VAHwYpXEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remove a row \n",
        "cradit_crd_df=cradit_crd_df.drop(cradit_crd_df.index[[0,0]],axis=0)"
      ],
      "metadata": {
        "id": "njPorUH3gxPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change columns names\n",
        "update_col=['ID','LIMIT_BAL','SEX','EDUCATION','MARRIAGE','AGE','PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6','PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6','DEF_PAY_NXT_MONTH']"
      ],
      "metadata": {
        "id": "wVvM3LfohB_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#update columns name\n",
        "cradit_crd_df.columns=update_col"
      ],
      "metadata": {
        "id": "gnebxqiKhdqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#updated dataset\n",
        "cradit_crd_df=cradit_crd_df.astype('int')"
      ],
      "metadata": {
        "id": "UI1mITW6ix72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cradit_crd_df.info()"
      ],
      "metadata": {
        "id": "Nkn2MT_eGrb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Simplifying column names and bringing consistency\n",
        "# cradit_crd_df.rename(\n",
        "#     columns={\"default payment next month\": \"DEFAULT\"}, inplace=True\n",
        "# )\n",
        "# cradit_crd_df.columns = [name.lower() for name in list(cradit_crd_df.columns)]"
      ],
      "metadata": {
        "id": "DyakBum68Npw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Checking the entries of the ID column\n",
        "# print(cradit_crd_df[\"id\"].value_counts())\n",
        "# cradit_crd_df.drop([\"id\"], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "P_xrfRGw8XA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Checking the range of the numerical entries\n",
        "# cradit_crd_df.describe(\n",
        "#     include=\"all\"\n",
        "# ).transpose() "
      ],
      "metadata": {
        "id": "Q4iSXkyZ9jMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "rows=cradit_crd_df.shape[0]\n",
        "columns=cradit_crd_df.shape[1]\n",
        "print(f\"The number of rows is {rows} and number of columns is {columns}.\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "cradit_crd_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "eLoOKbhlOaoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "condition = bool(cradit_crd_df.duplicated(subset = 'ID').any())\n",
        "\n",
        "if condition:\n",
        "    print('There are duplicate IDs')\n",
        "else:\n",
        "    print('No duplicate IDs')"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Identify outliers "
      ],
      "metadata": {
        "id": "gWbWS4GpO5Lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cradit_crd_df[\"LIMIT_BAL\"].plot(kind=\"box\")\n",
        "plt.xlabel('Credit limit in NT$', fontweight='bold')\n",
        "plt.ylabel('# of Customers', fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bRIuVKhpIUEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers = cradit_crd_df.loc[cradit_crd_df['LIMIT_BAL']>900000]\n",
        "outliers"
      ],
      "metadata": {
        "id": "RDFPm6HSPLE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "cradit_crd_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no NaN/NULL record in the dataset, So we dont have to impute any record."
      ],
      "metadata": {
        "id": "hSm60USbbov3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ey-5iGXVQTww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check numerical columns"
      ],
      "metadata": {
        "id": "IaNBe3CBQPEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "cradit_crd_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "UOiW-isgLCM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "IWp6SkWxLMuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the entries of the ID column\n",
        "print(cradit_crd_df[\"ID\"].value_counts())\n",
        "cradit_crd_df.drop([\"ID\"], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "EX3OMDG0dXCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "cradit_crd_df.head()"
      ],
      "metadata": {
        "id": "LaPZwvDHBvdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the statistic summary of the columns\n",
        "# No data is extremly unresonable in these columns\n",
        "# cradit_crd_df.describe()\n",
        "\n",
        "cradit_crd_df.describe(include='all' ).transpose()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. This study reviewed the literature and used the following 23 variables as explanatory variables:\n",
        "* X0 (ID): unique id value each row.\n",
        "\n",
        "* X1 (LIMIT_BAL): Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\n",
        "\n",
        "* X2 (SEX): Gender (1 = male; 2 = female).\n",
        "\n",
        "* X3 (EDUCATION): Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n",
        "\n",
        "* X4 (MARRIAGE): Marital status (1 = married; 2 = single; 3 = others).\n",
        "\n",
        "* X5 (AGE): Age (year).\n",
        "\n",
        "* X6 (PAY_1) - X11 (PAY_6): History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6(PAY_1) = the repayment status in September, 2005; X7(PAY_2) = the repayment status in August, 2005; . . ....;X11(PAY_6) = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n",
        "\n",
        "* X12 (BILL_AMT1)-X17 (BILL_AMT6): Amount of bill statement (NT dollar). X12(BILL_AMT1) = amount of bill statement in September, 2005; X13(BILL_AMT2) = amount of bill statement in August, 2005; . . .; X17(BILL_AMT6) = amount of bill statement in April, 2005.\n",
        "\n",
        "* X18(PAY_AMT1)-X23 (PAY_AMT6): Amount of previous payment (NT dollar). X18(PAY_AMT1) = amount paid in September, 2005; X19 (PAY_AMT2) = amount paid in August, 2005; . . .;X23(PAY_AMT6) = amount paid in April, 2005.\n",
        "* DEF_PAY_NXT_MONTH: default payment status next month.\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "pay=cradit_crd_df[['PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']]\n",
        "pay_melt=pd.melt(pay)\n",
        "print(pay_melt['value'].value_counts())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "TOHAaUzzKe61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_default_across_features(df, col_name, dict_label={}):\n",
        "    \"\"\"\n",
        "    This function quantifies and displays the distribution of default across the various classes of a feature.\n",
        "    \n",
        "    INPUT:\n",
        "    - df - (pd.DataFrame) the dataframe\n",
        "    - col_name - (str) the column name of the feature being considered\n",
        "    - dict_label - (dictionary) a dictionary relating the values of the column to what they represent\n",
        "    \n",
        "    OUTPUT:\n",
        "    - Displays a bar plot showing the population distribution by feature values\n",
        "    - Displays and returns a cross-tab showing the rate of default across each feature value\n",
        "    - Displays a bar plot showing the population distribution and default by the feature values \n",
        "    \"\"\"\n",
        "\n",
        "    # Create a cross-tab and rename indices for readability\n",
        "    cross_tab = pd.crosstab(df[\"DEF_PAY_NXT_MONTH\"], df[col_name], margins=True, normalize=False)\n",
        "\n",
        "    # new_index=['Not Default','Default']\n",
        "\n",
        "    new_index = {0: \"Non-default proportion\", 1: \"Default proportion\"}\n",
        "    # new_index={'non_default':0,'default':1}\n",
        "    new_columns = dict_label\n",
        "\n",
        "    cross_tab.rename(index=new_index, columns=new_columns, inplace=True)\n",
        "\n",
        "    # Plot a bar graph showing population distribution by the feature values\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    i = cross_tab.shape[1] - 1\n",
        "    cross_tab.loc[\"All\"][0:i].plot.bar(rot=30, fontsize=14)\n",
        "    plt.title(\"Population Distribution by \" + col_name, fontsize=20)\n",
        "    plt.ylabel(\"count\")\n",
        "    # plt.add_legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Normalise the cross-tab and print it\n",
        "    cross_tab_norm = cross_tab / cross_tab.loc[\"All\"]\n",
        "    display(cross_tab_norm)\n",
        "\n",
        "    # Plot a bar graph showing population distribution by the feature values separating the defaulters and non-defaulters\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    ax = sns.countplot(x=col_name, hue='DEF_PAY_NXT_MONTH',data=df)\n",
        "    plt.title(\n",
        "        \"Population Distribution by \" + col_name + \" with default and non-default\",\n",
        "        fontsize=20,\n",
        "    )\n",
        "\n",
        "    # Return the normalised cross-tab\n",
        "    return cross_tab_norm"
      ],
      "metadata": {
        "id": "nJpzr-J-7Mjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from locale import normalize\n",
        "# Get the proportion of customers who had default payment in the next month (Oct.2005)? \n",
        "# About 22% customers had default payment next month\n",
        "\n",
        "x=cradit_crd_df['DEF_PAY_NXT_MONTH'].value_counts(normalize=True)\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(x, colors=['springgreen', 'coral'], shadow=True, autopct='%1.2f%%', startangle=200)\n",
        "plt.legend(labels=['Not Default','Default'])\n",
        "plt.title(\" proportion of customers who had default payment in the next month\")"
      ],
      "metadata": {
        "id": "pAhPmnVtwPRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get SEX column's distribution. 1: male; 2: female\n",
        "# No undocumented SEX code\n",
        "cradit_crd_df[\"SEX\"].value_counts()"
      ],
      "metadata": {
        "id": "4xKWlkgLQJm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "x= compare_default_across_features(cradit_crd_df, 'SEX', {1: \"Male\", 2: \"Female\"})"
      ],
      "metadata": {
        "id": "rhSVGXxp7VfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although there are more female credit card holders, the default proportion among men is higher. I will do a hypothesis test to see if the difference is statistically significant.\n",
        "(1: default,0:non_default) \n",
        "Is default proportion affected by education?"
      ],
      "metadata": {
        "id": "y88ROKbY9vtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#From dataset description: EDUCATION: Education status (1=graduate school; 2 = university; 3 = high school; 4 = others).\n",
        "print(cradit_crd_df['EDUCATION'].unique())\n",
        "cradit_crd_df['EDUCATION'].value_counts()"
      ],
      "metadata": {
        "id": "ZJ1b63zwQiqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#But we get 6 values so, we have replace 5,6,0 values \n",
        "cradit_crd_df['EDUCATION']=cradit_crd_df['EDUCATION'].replace({0:4,5:4,6:4})\n",
        "cradit_crd_df['EDUCATION'].value_counts()"
      ],
      "metadata": {
        "id": "jPMmFuj1QuZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = compare_default_across_features(\n",
        "    cradit_crd_df,\n",
        "    \"EDUCATION\",\n",
        "    {\n",
        "        1: \"Grad School\",\n",
        "        2: \"University\",\n",
        "        3: \"High School\",\n",
        "        4: \"Others\",\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "pl9TQ4R084i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From dataset description: MARRIAGE: Marital status (1=married, 2=single, 3=others), but there is also 0\n",
        "print(cradit_crd_df['MARRIAGE'].value_counts())"
      ],
      "metadata": {
        "id": "N4pUsXZNRCDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we have replace 0 value.\n",
        "cradit_crd_df[\"MARRIAGE\"] = cradit_crd_df[\"MARRIAGE\"].replace({0:3})\n",
        "print(cradit_crd_df['MARRIAGE'].value_counts())"
      ],
      "metadata": {
        "id": "2KSA9rdQRsBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x= compare_default_across_features(\n",
        "    cradit_crd_df,\n",
        "    \"MARRIAGE\",\n",
        "    {\n",
        "        1: \"Married\",\n",
        "        2: \"Single\",\n",
        "        3: \"Others\",\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "G6VgK2709npp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Married people have higher default proportions than single folks. While there are intuitive arguments for and against it, closer inspection is needed. For example, is there a difference between married men and married women?"
      ],
      "metadata": {
        "id": "6jlmim34CrWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the age bins\n",
        "cradit_crd_df['age_group'] = cradit_crd_df['AGE']//10\n",
        "age_group_names = [str(i) + '0s' for i in range(2,8)]\n",
        "age_dict = dict(zip(range(2,8), age_group_names))\n",
        "#cc_df['age_bin'] = pd.cut(X['age'], 6, labels=age_group_names)"
      ],
      "metadata": {
        "id": "QjNoYltC_SS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_cross_tab = compare_default_across_features(cradit_crd_df, 'age_group', age_dict)"
      ],
      "metadata": {
        "id": "_KdzWQmJ_i26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a boxplot to visualize credit limit and default payment next month\n",
        "# 1: default next month; 0: no default next month\n",
        "# cradit_crd_def0 = cradit_crd_df.loc[cradit_crd_df['DEF_PAY_NXT_MONTH'] == 0][\"LIMIT_BAL\"]\n",
        "# cradit_crd_def1 = cradit_crd_df[cradit_crd_df['DEF_PAY_NXT_MONTH'] == 1][\"LIMIT_BAL\"]\n",
        "# plt.figure(figsize = (14,6))\n",
        "# sns.distplot(cradit_crd_def0,kde=True,bins=200, label='Non-default')\n",
        "# sns.distplot(cradit_crd_def1,kde=True,bins=200, label=\"Default\")\n",
        "# plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), fontsize=14)\n",
        "# plt.title('Credit Limit Amount (Density Plot)')\n",
        "# plt.xlabel('Credit Limit Amount (in NT dollars)')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "ZOgqpshqMlMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the density plot, it seems that people with higher credit limit have significantly lower default proportion. Again, intuitively that is not surprising because the people who have higher credit limits must have displayed long periods of fiscal responsibility to reach that place."
      ],
      "metadata": {
        "id": "YDKzqLOXNyJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trends, Hypotheses and Findings"
      ],
      "metadata": {
        "id": "WiNgyT_sTdts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a boxplot to visualize credit limit and default payment next month\n",
        "# 1: default next month; 0: no default next month\n",
        "def0 = cradit_crd_df.loc[cradit_crd_df['DEF_PAY_NXT_MONTH'] == 0,'LIMIT_BAL']\n",
        "def1 = cradit_crd_df.loc[cradit_crd_df['DEF_PAY_NXT_MONTH'] == 1,'LIMIT_BAL']\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.boxplot([def0, def1],  showfliers=False)\n",
        "ax.set_xticklabels(['No_default',\"Default\"],fontweight ='bold')\n",
        "ax.set_ylabel('Credit limit',fontweight ='bold')\n",
        "ax.set_title('Credit limit & default next month',fontweight ='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PDSS_AHmz_lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the plot, it seems that people with higher credit limit have significantly lower default proportion. Again, intuitively that is not surprising because the people who have higher credit limits must have displayed long periods of fiscal responsibility to reach that place."
      ],
      "metadata": {
        "id": "lYuRVFCZ0cG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cradit_crd_df.head().transpose()"
      ],
      "metadata": {
        "id": "tJt2gOslvnQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get statistic summary of bill statement columns\n",
        "# The min numbers are negative\n",
        "bill = cradit_crd_df[['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']]\n",
        "bill.describe(include='all').transpose()"
      ],
      "metadata": {
        "id": "OhBnK4NxvKHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Subset a dataframe with the records that have default\n",
        "has_default = cradit_crd_df[cradit_crd_df['DEF_PAY_NXT_MONTH']== 1]\n",
        "\n",
        "default_trend = has_default[['PAY_6','PAY_5','PAY_4','PAY_3','PAY_2','PAY_1']].sum(axis=0)\n",
        "\n",
        "# Draw a line chart to show the trend. The lower the number, the shorter delayed payment\n",
        "fig,ax = plt.subplots()\n",
        "ax.plot(default_trend)\n",
        "plt.xticks(['PAY_6','PAY_5','PAY_4','PAY_3','PAY_2','PAY_1'],['Apr','May','Jun','Jul','Aug','Sep'])\n",
        "\n",
        "plt.xlabel('Months in 2005',fontweight='bold')\n",
        "plt.ylabel('Total delayed months',fontweight='bold')\n",
        "plt.title('Delayed payment trend',fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gP8We1rf_Qsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Is there any bill amount that is greater than credit limit?\n",
        "condition1 = cradit_crd_df['BILL_AMT1'] > cradit_crd_df['LIMIT_BAL'] \n",
        "condition2 = cradit_crd_df['BILL_AMT2'] > cradit_crd_df['LIMIT_BAL'] \n",
        "condition3 = cradit_crd_df['BILL_AMT3'] > cradit_crd_df['LIMIT_BAL'] \n",
        "condition4 = cradit_crd_df['BILL_AMT4'] > cradit_crd_df['LIMIT_BAL'] \n",
        "condition5 = cradit_crd_df['BILL_AMT5'] > cradit_crd_df['LIMIT_BAL'] \n",
        "condition6 = cradit_crd_df['BILL_AMT6'] > cradit_crd_df['LIMIT_BAL'] \n",
        "\n",
        "large_bill = cradit_crd_df[condition1 | condition2 |condition3 | condition4 | condition5 | condition6]\n",
        "large_bill['DEF_PAY_NXT_MONTH'].value_counts(normalize=True)\n",
        "# x=cradit_crd_df['DEF_PAY_NXT_MONTH'].value_counts(normalize=True)\n"
      ],
      "metadata": {
        "id": "pWBDqQtz_7EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column \"HAS_DEF\" to indicate customers who have at least 1 default payment from PAY_1 to Pay_6\n",
        "# 0 : no default ; 1: has default\n",
        "\n",
        "def_condition =(cradit_crd_df.PAY_1>1) | (cradit_crd_df.PAY_2>1) | (cradit_crd_df.PAY_3>1) | (cradit_crd_df.PAY_4>1) | (cradit_crd_df.PAY_5>1) | (cradit_crd_df.PAY_6>1)\n",
        "cradit_crd_df.loc[def_condition, \"HAS_DEF\"] = 1\n",
        "cradit_crd_df.loc[cradit_crd_df.HAS_DEF.isna(), \"HAS_DEF\"] = 0\n"
      ],
      "metadata": {
        "id": "FUHgSowSTZpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cradit_crd_df.head()"
      ],
      "metadata": {
        "id": "OVvuVNRDau8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# Plotting the bill amount density plots and their scatter plots\n",
        "\n",
        "sns.pairplot(cradit_crd_df, vars=cradit_crd_df.columns[11:17], kind='scatter',hue= 'HAS_DEF')"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of the bill amounts are skewed. If the model assumptions require symmetric/normal distributions, a log transformation or a Box-Cox transformation might be warranted.\n",
        "\n",
        "I will be using a MinMaxScaler later to scale the data given the presence of a lot of outliers."
      ],
      "metadata": {
        "id": "x39aY_ptcFsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr = cradit_crd_df.corr()\n",
        "\n",
        "plt.figure(figsize=(18, 15))\n",
        "sns.heatmap(corr, annot=True, vmin=-1.0, cmap='mako')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RS5dsMgMr0qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistical Inference"
      ],
      "metadata": {
        "id": "kOWpNS_uDEgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Is Default Affected by Gender?"
      ],
      "metadata": {
        "id": "pxEVg-iqDMdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does gender affect the default rate? I will try to answer this question with a hypothesis test. As is customary, I'll use a significance level of α=0.05\n",
        ". Then the bounds of the confidence interval are given by [α/2,1-α/2]=[0.25,0.975].\n",
        " \n",
        " \n"
      ],
      "metadata": {
        "id": "rWWapurRDTIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.05\n",
        "lb, ub = (alpha / 2), 1 - (alpha / 2)\n",
        "ci_bounds = [lb, ub]"
      ],
      "metadata": {
        "id": "SZRwj2ukDBsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to test whether the proportion <b>Pm</b>\n",
        " of men defaulting is the same as the proportion <b>Pw</b>\n",
        " of women defaulting. Hence, my test statistic is the difference between <b>Pw</b>\n",
        " and <b>Pm</b>\n",
        ".\n",
        "\n",
        "I state the null Hypothesis and alternate Hypothesis.\n",
        "* H0: Pm=Pw\n",
        "\n",
        "* Ha: Pm≠Pw\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement "
      ],
      "metadata": {
        "id": "POgV1Pe6JrRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "HXkFub4QJzOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "ttest, pval = stats.ttest_ind(cradit_crd_df[cradit_crd_df['SEX'] == 1]['HAS_DEF'], cradit_crd_df[cradit_crd_df['SEX'] == 2]['HAS_DEF'], equal_var=False)"
      ],
      "metadata": {
        "id": "MUheRvKgGRyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to check the Pval is grater then 0.05\n",
        "print(pval)\n",
        "if pval<0.05:\n",
        "  print(\"Null Hypothesis is rejected\")\n",
        "else:\n",
        "  print(\"Alternate Hypothesis is rejected\")  "
      ],
      "metadata": {
        "id": "fketmG1XI4N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results the null hypotheis of Pm=Pw is rejected. "
      ],
      "metadata": {
        "id": "DFz6luljJWHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on my understanding of the data and conclusions from EDA, I engineer a few features."
      ],
      "metadata": {
        "id": "-sOxsv_56oAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having engineered the features, I see if some of the aggregated columns are correlated with default. A quick look does show some relation although I would have to do some statistical tests to see if that is significant.\n",
        "\n",
        "I draw a scatter plot of average payment amount and average bill amount and notice that there is a marked difference in slope in the line of best fit for the default and non-default class."
      ],
      "metadata": {
        "id": "CfI9F8ybIYB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I notice that about one third of the people whose average bill amount was more than the credit limit defaulted. As a result, I engineer a feature called overdraft which takes value 1 if the user defaulted at any point in the past 6 months."
      ],
      "metadata": {
        "id": "ywWkPMx6L7j6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the original data set, along with some minor data adjustments discussed earlier and the one-hot encoding of the marriage feature."
      ],
      "metadata": {
        "id": "1k9PXrohY3S6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset contains features that avoid multicollinearity in the model (and hence, statistically significant non-zero coefficients in a logistic model). I decide on this list by looking at the vif and deviance reduction and also at the feature importances of the models. The dataset is then scaled and the training set balanced.\n",
        "\n"
      ],
      "metadata": {
        "id": "FaLhHeUWZV7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing \n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cradit_crd_df.info()"
      ],
      "metadata": {
        "id": "YfkRcoRxk1BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# Define predictor variables and target variable\n",
        "X = cradit_crd_df.drop(columns=['DEF_PAY_NXT_MONTH'])\n",
        "y = cradit_crd_df['DEF_PAY_NXT_MONTH']\n",
        "\n",
        "# Save all feature names as list\n",
        "feature_cols = X.columns.tolist() \n",
        "#print(feature_cols)\n",
        "# Extract numerical columns and save as a list for rescaling\n",
        "X_num = X.drop(columns=['SEX', 'EDUCATION', 'MARRIAGE', 'AGE'])\n",
        "num_cols = X_num.columns.tolist() \n",
        "print(num_cols)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check target classes balancec\n",
        "cla_bal = cradit_crd_df['DEF_PAY_NXT_MONTH'].value_counts(normalize=True)\n",
        "print(cla_bal)\n",
        "# Plot the classes\n",
        "cla_bal.plot(kind = 'bar')\n",
        "plt.title('Nondefault(0) and default(1) comparison',fontweight = \"bold\")\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Percentage')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zvjVmX4-gkhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define a function that count for imbalances\n",
        "\n",
        "def data_split(x,y,imbalance=False):\n",
        "  '''\n",
        "This function will split the data according to the imbalance in the data set \n",
        "if imbalance is there in then use SMOTE Analysis   '''\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,shuffle=True, stratify=y, random_state=42)\n",
        "  if imbalance:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    sm = SMOTE(random_state = 42)\n",
        "    X_train, y_train = sm.fit_resample(X_train, y_train.ravel())  \n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "0Y0mKAwC6hOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to rescale training data using StandardScaler\n",
        "def rescaling(X_train, X_test, numerical_cols):\n",
        "    \n",
        "    # Make copies of dataset\n",
        "    X_train_std = X_train.copy()\n",
        "    X_test_std = X_test.copy()\n",
        "    \n",
        "    # Apply standardization on numerical features only\n",
        "    for i in numerical_cols:\n",
        "        scl = StandardScaler().fit(X_train_std[[i]])     # fit on training data columns\n",
        "        X_train_std[i] = scl.transform(X_train_std[[i]]) # transform the training data columns\n",
        "        X_test_std[i] = scl.transform(X_test_std[[i]])   # transform the testing data columns\n",
        "    \n",
        "    return X_train_std,X_test_std"
      ],
      "metadata": {
        "id": "nCx_JUIM6z-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_recall(model,X_test,y_test):\n",
        "  y_pred=model.predict(X_test)\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "    \n",
        "  precision = tp / (tp + fp)\n",
        "  recall = tp / (tp + fn)\n",
        "  F1 = 2 * (precision * recall) / (precision + recall)\n",
        "  print(f'Precision:{precision:.3f}\\nRecall:{recall:.3f}\\nF1 score:{F1:.3f}')"
      ],
      "metadata": {
        "id": "zowEhTzBQhbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(imbalance=False):\n",
        "  X_train, X_test, y_train, y_test=data_split(X,y,imbalance=imbalance)\n",
        "  X_train_std,X_test_std=rescaling(X_train,X_test,numerical_cols = num_cols)\n",
        "  # print(X_train_std)\n",
        "  clf_lr = LogisticRegression(random_state=42)\n",
        "  clf_lr.fit(X_train_std, y_train)\n",
        "  scores = cross_val_score(clf_lr, X_train_std, y_train, scoring =\"roc_auc\", cv = 5)\n",
        "  roc_auc_lr = np.mean(scores)\n",
        "  # print(roc_auc_lr)\n",
        "\n",
        "  if imbalance:\n",
        "    return \"Logistic Regression\",\"With SMOTE\",roc_auc_lr\n",
        "  else:\n",
        "    return \"Logistic Regression\",\"Without SMOTE\",roc_auc_lr\n",
        "    \n",
        "model_result=[]\n",
        "# y=y.astype('int')\n",
        "model_result.append(logistic_regression())    \n",
        "model_result.append(logistic_regression(imbalance=True))\n",
        "pd.DataFrame(model_result,columns=['Model','Smote','ROC_AUC'])"
      ],
      "metadata": {
        "id": "2Fxb5fh_7Mi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomized search for the best C parameter\n",
        "# Split data with SMOTE \n",
        "X_train, X_test, y_train, y_test = data_split(X, y, imbalance = True) \n",
        "\n",
        "# Rescale data\n",
        "X_train_std, X_test_std = rescaling(X_train, X_test, numerical_cols = num_cols)\n",
        "# X_train_std, X_test_std=X_train_std.astype('int')\n",
        "\n",
        "logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,random_state=42)\n",
        "distributions = dict(C=uniform(loc=0, scale=4), penalty=['l2', 'l1','elasticnet'])\n",
        "clf = RandomizedSearchCV(logistic, distributions, random_state=42)\n",
        "\n",
        "lr_best= clf.fit(X_train_std, y_train)   \n",
        "#print(distributions)\n",
        "print(lr_best.best_params_)"
      ],
      "metadata": {
        "id": "SrafwlUjQ890"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(lr_best, X_train_std, y_train, scoring =\"roc_auc\", cv = 5)\n",
        "roc_auc_lr = np.mean(scores)\n",
        "print(f'Roc_Auc score for the Logistic regression with SMOTE :{roc_auc_lr,\".3f\"}')"
      ],
      "metadata": {
        "id": "Ss7SPW23RO2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_recall(lr_best,X_test_std,y_test)     "
      ],
      "metadata": {
        "id": "28GUHKLSRawv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data with SMOTE\n",
        "X_train_sm, X_test, y_train_sm, y_test = data_split(X, y, imbalance = True)"
      ],
      "metadata": {
        "id": "ivllibvviOBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create parameter grid  \n",
        "param_grid = {\n",
        "    'max_depth': [60, 90, 110],\n",
        "    'min_samples_leaf': [3, 4, 5],\n",
        "    'min_samples_split': [8, 10, 12],\n",
        "    'n_estimators': [100, 200, 300]\n",
        "}\n",
        "\n",
        "# Instantiate the model\n",
        "clf_rf = RandomForestClassifier()\n",
        "\n",
        "# Instantiate grid search model\n",
        "grid_search = GridSearchCV(estimator = clf_rf, param_grid = param_grid,    \n",
        "                          cv = 2, n_jobs = -1, verbose = 1)\n",
        "\n",
        "# Fit grid search to the data\n",
        "grid_search.fit(X_train_sm, y_train_sm)\n",
        "grid_search.best_params_"
      ],
      "metadata": {
        "id": "n8XhYnqriWIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "GXi--n2LilL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_best = RandomForestClassifier(**grid_search.best_params_)   \n",
        "rf_best.fit(X_train_sm,y_train_sm)\n",
        "\n",
        "scores_best = cross_val_score(rf_best, X_train_sm, y_train_sm, scoring =\"roc_auc\", cv = 3)\n",
        "roc_auc_best = np.mean(scores_best)\n",
        "\n",
        "print(f'ROC_AUC training score after tuning for Random Forest: {roc_auc_best:.3f}')"
      ],
      "metadata": {
        "id": "eT_Rzo-ZioXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The F1 score,Precision and Recall value for Random Forest :\")\n",
        "precision_recall(rf_best,X_test,y_test)  \n",
        "# precision_score(X_test,y_test)"
      ],
      "metadata": {
        "id": "CY1tIdqTivRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# change karna hain"
      ],
      "metadata": {
        "id": "MsLVJy8e2uGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xgboost(imbalance=False):\n",
        "   X_train, X_test, y_train, y_test=data_split(X,y,imbalance=imbalance)\n",
        "   X_train_std,X_test_std=rescaling(X_train,X_test,numerical_cols = num_cols)\n",
        "   clf_lr = XGBClassifier(random_state=42)\n",
        "   clf_lr.fit(X_train_std, y_train)\n",
        "  #  model_xgb.fit(X_train, y_train)\n",
        "   scores = cross_val_score(clf_lr, X_train_std, y_train, scoring =\"roc_auc\", cv = 5)\n",
        "   roc_auc_lr = np.mean(scores)\n",
        "  #  print(roc_auc_lr)\n",
        "   if imbalance:\n",
        "     return \"XGBOOST\",\"With SMOTE\",roc_auc_lr\n",
        "   else:\n",
        "     return \"XGBOOST\",\"Without SMOTE\",roc_auc_lr    \n",
        "model_result=[]\n",
        "# y=y.astype('int')\n",
        "model_result.append(xgboost())    \n",
        "model_result.append(xgboost(imbalance=True))\n",
        "pd.DataFrame(model_result,columns=['Model','SMOTE','ROC_AUC'])"
      ],
      "metadata": {
        "id": "DCbt5dyPYjvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data with SMOTE\n",
        "X_train_sm, X_test, y_train_sm, y_test = data_split(X, y, imbalance = True)"
      ],
      "metadata": {
        "id": "GWL1MSqcICAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from skopt.space import Bayes searchCV\n",
        "# from skopt.space import real,categorical,integer\n",
        "params = { \n",
        "    'gamma':0,\n",
        "    'learning_rate':0.01, \n",
        "    'max_depth':3, \n",
        "    'colsample_bytree':0.6,\n",
        "    'subsample':0.8,\n",
        "    'scale_pos_weight':3.5,\n",
        "    'n_estimators':1000,\n",
        "    'objective':'binary:logistic', \n",
        "    'reg_alpha':0.3    \n",
        "}\n",
        "\n",
        "clf_xgb=XGBClassifier(**params)\n",
        "scores_best = cross_val_score(clf_xgb, X_train_sm, y_train_sm, scoring =\"roc_auc\", cv = 3)\n",
        "roc_auc_best = np.mean(scores_best)\n",
        "print(f'ROC_AUC training score after tuning for initial parameter in XGBOOST: {roc_auc_best:.3f}')"
      ],
      "metadata": {
        "id": "3RMuF7VrIHLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The F1 score, Precision and Recall for XGBOOST is :\")\n",
        "# precision_recall(xgb_random,X_test,y_test)"
      ],
      "metadata": {
        "id": "T0-hIFR_YQsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#kagggle"
      ],
      "metadata": {
        "id": "qJhsEFlCmQO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score"
      ],
      "metadata": {
        "id": "ZSpITq9znPSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models={\n",
        "    LogisticRegression():'Logistic Regression',\n",
        "    SVC(): \"Support Vector Machine\",\n",
        "    MLPClassifier(): \"Naural Network\" ,\n",
        "    RandomForestClassifier(): \"Random Forest Classifier\",\n",
        "    # XGBClassifier():  \"XGB Classifier\"\n",
        "}\n",
        "for model in models.keys():\n",
        "  model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "qjm5aRUfmO1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model, name in models.items():\n",
        "  print(name + \": {:.2f}%\".format(model.score(X_test, y_test) * 100))"
      ],
      "metadata": {
        "id": "mapTULBXoR7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2 - XGB Classifier\n",
        "# x=X_train.astype('int')\n",
        "# y=y_train.astype('int')\n",
        "model_xgb = XGBClassifier()\n",
        "model_xgb.fit(X_train, y_train)\n",
        "print(\"Xgboost Accuracy =\",accuracy_score(y_train, model_xgb.predict(X_train)))"
      ],
      "metadata": {
        "id": "CFZ2dUucuXM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Classifier\n",
        "classifier_dtc = DecisionTreeClassifier()\n",
        "classifier_dtc.fit(X_train, y_train)\n",
        "# Predicting the Test set results\n",
        "y_pred= classifier_dtc.predict(X_test)\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "clf_rpt = classification_report(y_test,y_pred)\n",
        "print(\"classification report :\", clf_rpt)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm,annot=True)"
      ],
      "metadata": {
        "id": "86stz1GJ_tsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hyperparameter tuning\n",
        "\n",
        "classifier = RandomForestClassifier() # For GBM, use GradientBoostingClassifier()\n",
        "grid_values = {'n_estimators':[50, 80,  100], 'max_depth':[3, 5, 7]}\n",
        "classifier = GridSearchCV(classifier, param_grid = grid_values, scoring = 'roc_auc', cv=5)\n",
        "\n",
        "# Fit the object to train dataset\n",
        "classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "-iQJOWUJHSZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_preds =  classifier.predict(X_train)\n",
        "test_preds  = classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "OqfZqPVuHcbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain accuracy on train set\n",
        "accuracy_score(y_train,train_preds)"
      ],
      "metadata": {
        "id": "ptqeQb_iHdwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain accuracy on test set\n",
        "accuracy_score(y_test,test_preds)"
      ],
      "metadata": {
        "id": "OCZCCZY3HjNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate roc_auc score on train set\n",
        "roc_auc_score(y_train,train_preds)"
      ],
      "metadata": {
        "id": "1F3YpM8UHqn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate roc_auc score on test set\n",
        "roc_auc_score(y_test,test_preds)"
      ],
      "metadata": {
        "id": "hqfndgXeHtti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# features = X_train.columns\n",
        "# importances = classifier.rf_best.feature_importances_\n",
        "# indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "l7ZyQfR_HBnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot feature importance of winner model - Random Forest\n",
        "\n",
        "fea_df = pd.DataFrame({'Feature': feature_cols, 'Feature importance': rf_best.feature_importances_})\n",
        "fea_df = fea_df.sort_values(by='Feature importance')\n",
        "\n",
        "figure, ax = plt.subplots(figsize = (10,8))\n",
        "fea_df.plot.barh(x='Feature',y='Feature importance', ax=ax)\n",
        "plt.title('Features importance',fontsize=14)\n",
        "     "
      ],
      "metadata": {
        "id": "4mEbnypNJ2vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# # Fit the Algorithm\n",
        "# reg_model=LogisticRegression(random_state=32)\n",
        "# reg_model.fit(xtrain,ytrain['credit_card_default'])\n",
        "# predicted_val= reg_model.predict(xval)\n",
        "# score_train=100*f1_score(predicted_val,yval['credit_card_default'],average='macro')\n",
        "# #model.score(xval,yval)\n",
        "# print(score_train)\n",
        "# # Predict on the model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}